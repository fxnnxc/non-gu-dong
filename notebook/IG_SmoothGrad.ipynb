{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Get the MNIST DATA\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to untracked/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:01, 5978787.37it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting untracked/MNIST/raw/train-images-idx3-ubyte.gz to untracked/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to untracked/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 176271.86it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting untracked/MNIST/raw/train-labels-idx1-ubyte.gz to untracked/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to untracked/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:01, 1169016.88it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting untracked/MNIST/raw/t10k-images-idx3-ubyte.gz to untracked/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to untracked/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 21410604.67it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting untracked/MNIST/raw/t10k-labels-idx1-ubyte.gz to untracked/MNIST/raw\n",
      "\n",
      "[+] Finished loading data & Preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as vision_dsets\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as T # Transformation functions to manipulate images\n",
    "\n",
    "\n",
    "def get_mnist_dataloader(root='untracked',train =True,transforms=T.ToTensor() ,download =True,batch_size = 32,num_worker = 1):\n",
    "    print (\"[+] Get the MNIST DATA\")\n",
    "    \"\"\"\n",
    "    We will use Mnist data for our tutorial \n",
    "    \"\"\"\n",
    "    mnist_train = vision_dsets.MNIST(root = root,  #root is the place to store your data. \n",
    "                                    train = True,  \n",
    "                                    transform = transforms,\n",
    "                                    download=download)\n",
    "    mnist_test = vision_dsets.MNIST(root = root,\n",
    "                                    train = False, \n",
    "                                    transform = transforms,\n",
    "                                    download=download)\n",
    "    \"\"\"\n",
    "    Data Loader is a iterator that fetches the data with the number of desired batch size. \n",
    "    * Practical Guide : What is the optimal batch size? \n",
    "      - Usually.., higher the batter. \n",
    "      - We recommend to use it as a multiple of 2 to efficiently utilize the gpu memory. (related to bit size)\n",
    "    \"\"\"\n",
    "    trainDataLoader = data.DataLoader(dataset = mnist_train,  # information about your data type\n",
    "                                      batch_size = batch_size, # batch size\n",
    "                                      shuffle =True, # Whether to shuffle your data for every epoch. (Very important for training performance)\n",
    "                                      num_workers = 1) # number of workers to load your data. (usually number of cpu cores)\n",
    "\n",
    "    testDataLoader = data.DataLoader(dataset = mnist_test, \n",
    "                                    batch_size = batch_size,\n",
    "                                    shuffle = False, # we don't actually need to shuffle data for test\n",
    "                                    num_workers = 1) #\n",
    "    print (\"[+] Finished loading data & Preprocessing\")\n",
    "    return trainDataLoader,testDataLoader\n",
    "\n",
    "trainDataLoader, testDataLoader = get_mnist_dataloader(transforms=T.Compose([T.ToTensor(), ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model=\"linear\"):\n",
    "        super().__init__()\n",
    "        self.model_type = model\n",
    "        if model ==\"linear\":\n",
    "            self.linear1 = nn.Linear(784, 128)\n",
    "            self.linear2 = nn.Linear(128, 10)\n",
    "        elif model == \"cnn\":\n",
    "            self.conv1 = nn.Conv2d(in_channels= 1, out_channels=32, kernel_size=3, stride=1)\t\n",
    "            self.conv2 = nn.Conv2d(in_channels=32, out_channels=28, kernel_size=3, stride=2)\n",
    "            self.fc1 = nn.Linear(in_features=4032, out_features=512)\n",
    "            self.fc2 = nn.Linear(in_features=512, out_features=10)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        if self.model_type == \"linear\":\n",
    "            x = x.view(batch_size, -1)\n",
    "            x = self.linear1(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.linear2(x) \n",
    "        else:\n",
    "\n",
    "            x = nn.functional.relu(self.conv1(x))\n",
    "            x = nn.functional.relu(self.conv2(x))\n",
    "            x = x.contiguous().view(batch_size, -1)\n",
    "            x = self.fc1(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = Model(model=\"linear\")\n",
    "cnn_model = Model(model=\"cnn\")\n",
    "\n",
    "# Todo : Cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 1.748\n",
      "[1,  1000] loss: 0.763\n",
      "[1,  1500] loss: 0.527\n",
      "[2,   500] loss: 0.415\n",
      "[2,  1000] loss: 0.385\n",
      "[2,  1500] loss: 0.354\n",
      "[1,   500] loss: 1.403\n",
      "[1,  1000] loss: 0.344\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def train(net, train_loader, optimizer, criterion,  epoch=2):\n",
    "    net.train()\n",
    "    for e in range(epoch):\n",
    "        running_loss = 0.0  \n",
    "        for i, data in enumerate(train_loader, 0): \n",
    "            # get the inputs\n",
    "            inputs, labels = data # Return type for data in dataloader is tuple of (input_data, labels)\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()    \n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs) # get output after passing through the network\n",
    "            loss = criterion(outputs, labels) # compute model's score using the loss function \n",
    "            loss.backward() # perform back-propagation from the loss\n",
    "            optimizer.step() # perform gradient descent with given optimizer\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 500 == 0:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (e + 1, i + 1, running_loss / 500))\n",
    "                running_loss = 0.0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "models = [linear_model, cnn_model]\n",
    "optimizers  = [torch.optim.SGD(linear_model.parameters(), lr=0.001, momentum=0.9), \n",
    "                torch.optim.SGD(cnn_model.parameters(), lr=0.001, momentum=0.9)]\n",
    "\n",
    "\n",
    "for model, opti, in zip(models, optimizers):\n",
    "    print(\"--- Training Started with model ---\")\n",
    "    print(model)\n",
    "    train(model, trainDataLoader, opti, criterion, 2)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "728ed223cfa85ea1ef5dcc6c79a939ffd9902707d91f95b40f547e46903ca84f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
